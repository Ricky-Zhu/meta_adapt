lora_rank: 8
train_all_bias: false
adapt_demo_num_each_task: 5
adaptation_task_id: 5
n_epochs: 50
eval_every: 10
eval: false
n_eval: 20
eval_in_train: false
exp_dir: './experiments/lora_adaptation/'
pre_trained_model_path: '../scripts/experiments/LIBERO_OBJECT/PreTrainMultitask/BCTransformerPolicy_seed10000/run_003/multitask_model.pth'
seed: 100
policy_type: 'LoraBCTPolicy'
batch_size: 16
optim_name: torch.optim.AdamW

optim_kwargs:
    lr: 0.0003
    betas: [0.9, 0.999]
    weight_decay: 0.0001
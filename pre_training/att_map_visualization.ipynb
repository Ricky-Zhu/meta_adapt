{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# load the trajectory dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d856eeebb49df6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import h5py\n",
    "import numpy as np\n",
    "from libero.lifelong.datasets import *\n",
    "from libero.libero.utils.dataset_utils import get_dataset_info\n",
    "from IPython.display import HTML\n",
    "import imageio\n",
    "from libero.libero import benchmark, get_libero_path, set_libero_default_path\n",
    "import os\n",
    "from termcolor import colored\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import cv2\n",
    "from libero.lifelong.models import *\n",
    "from libero.lifelong.utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:17.552057Z",
     "start_time": "2024-06-17T10:25:15.962813Z"
    }
   },
   "id": "c0d0199cfc0264e8",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/home/ruiqi/projects/meta_adapt/scripts/experiments/LIBERO_OBJECT/PreTrainMultitask/BCViLTPolicy_seed10000/run_001/multitask_model_ep10.pth'\n",
    "checkpoint = torch.load(model_path)\n",
    "sd = checkpoint['state_dict']\n",
    "cfg = checkpoint['cfg']\n",
    "model = get_policy_class(cfg.policy.policy_type)(cfg, cfg.shape_meta)\n",
    "model.load_state_dict(sd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.264605Z",
     "start_time": "2024-06-17T10:25:17.553529Z"
    }
   },
   "id": "f22d89bfe5f0c38d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "datasets_default_path = get_libero_path(\"datasets\")\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "benchmark_instance = benchmark_dict[\"libero_object\"]()\n",
    "num_tasks = benchmark_instance.get_num_tasks()\n",
    "demo_files = [os.path.join(datasets_default_path, benchmark_instance.get_task_demonstration(i)) for i in\n",
    "              range(num_tasks)]\n",
    "for demo_file in demo_files:\n",
    "    if not os.path.exists(demo_file):\n",
    "        print(colored(f\"[error] demo file {demo_file} cannot be found. Check your paths\", \"red\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.270155Z",
     "start_time": "2024-06-17T10:25:23.265729Z"
    }
   },
   "id": "7365b44e18e8c14e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_dataset(task_id, traj_id):\n",
    "    example_demo_file = demo_files[task_id]\n",
    "    modaility = {'rgb': ['agentview_rgb', 'eye_in_hand_rgb'],\n",
    "                 'depth': [],\n",
    "                 'low_dim': ['gripper_states', 'joint_states']}\n",
    "\n",
    "    task_dataset, shape_meta = get_dataset(\n",
    "        dataset_path=example_demo_file,\n",
    "        obs_modality=modaility,\n",
    "        initialize_obs_utils=True,\n",
    "        seq_len=1,\n",
    "    )\n",
    "    descriptions = []\n",
    "    task_description = benchmark_instance.get_task(task_id).language\n",
    "    descriptions.append(task_description)\n",
    "    task_embs = get_task_embs(cfg, descriptions)\n",
    "\n",
    "    traj_data = task_dataset.get_trajectory_at_index(traj_id)\n",
    "    agentview_data = traj_data['obs']['agentview_rgb']\n",
    "    eye_in_hand_rgb_data = traj_data['obs']['eye_in_hand_rgb']\n",
    "\n",
    "    # load the agent view list\n",
    "    agentview_data_list = []\n",
    "    agentview_data_list.append(torch.from_numpy(agentview_data[0]))\n",
    "    for i in range(1, len(agentview_data) - 1, 10):\n",
    "        agentview_data_list.append(torch.from_numpy(agentview_data[i]))\n",
    "    agentview_data_list.append(torch.from_numpy(agentview_data[-1]))\n",
    "\n",
    "    # load the eye in hand view list\n",
    "    eye_in_hand_data_list = []\n",
    "    eye_in_hand_data_list.append(torch.from_numpy(eye_in_hand_rgb_data[0]))\n",
    "    for i in range(1, len(eye_in_hand_rgb_data) - 1, 10):\n",
    "        eye_in_hand_data_list.append(torch.from_numpy(eye_in_hand_rgb_data[i]))\n",
    "    eye_in_hand_data_list.append(torch.from_numpy(eye_in_hand_rgb_data[-1]))\n",
    "    return task_embs, agentview_data_list, eye_in_hand_data_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.276146Z",
     "start_time": "2024-06-17T10:25:23.270971Z"
    }
   },
   "id": "b29e0d496e41dc53",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# load the model and fetch the att maps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47cba57e7db2918d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def construct_data(task_emb, agent_view_im, eye_in_hand_im):\n",
    "    data = {}\n",
    "    data['obs'] = {}\n",
    "    data['obs']['agentview_rgb'] = agent_view_im[None, None, :]\n",
    "    data['obs']['eye_in_hand_rgb'] = eye_in_hand_im[None, None, :]\n",
    "    data['task_emb'] = task_emb[None, :]\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.281018Z",
     "start_time": "2024-06-17T10:25:23.277299Z"
    }
   },
   "id": "1d7b09c0f97a911b",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# deal with the att maps "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32238dc499bda190"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_att_im(data, model):\n",
    "    att_maps = model.get_spatial_summary(data)\n",
    "    att_maps_all_layer = []\n",
    "    for i in range(len(att_maps)):\n",
    "        att_maps_all_layer.append(att_maps[i])\n",
    "    att_mat = torch.stack(att_maps_all_layer).squeeze(1)\n",
    "\n",
    "    # Average the attention weights across all heads.\n",
    "    att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n",
    "\n",
    "    v = joint_attentions[-1]\n",
    "\n",
    "    # get the att of the spatial summary token (index 0) over all other tokens\n",
    "    agentview_att = v[0, 1:65]\n",
    "    agentview_att = agentview_att / agentview_att.sum()\n",
    "    # print(agentview_att.sum())\n",
    "    eye_in_hand_att = v[0, 65:65 + 64]\n",
    "    eye_in_hand_att = eye_in_hand_att / eye_in_hand_att.sum()\n",
    "    grid_size = 8\n",
    "\n",
    "    im_agentview = data['obs']['agentview_rgb'][0, 0, :].numpy().transpose(1, 2, 0)\n",
    "    im_eye_in_hand = data['obs']['eye_in_hand_rgb'][0, 0, :].numpy().transpose(1, 2, 0)\n",
    "\n",
    "    mask_agentview = agentview_att.reshape(grid_size, grid_size).detach().numpy()\n",
    "    mask_agentview = cv2.resize(mask_agentview / mask_agentview.max(), im_agentview.shape[:-1])[..., np.newaxis]\n",
    "\n",
    "    mask_eye_in_hand = eye_in_hand_att.reshape(grid_size, grid_size).detach().numpy()\n",
    "    mask_eye_in_hand = cv2.resize(mask_eye_in_hand / mask_eye_in_hand.max(), im_eye_in_hand.shape[:-1])[..., np.newaxis]\n",
    "\n",
    "    masked_im_a = im_agentview * mask_agentview\n",
    "    masked_im_e = im_eye_in_hand * mask_eye_in_hand\n",
    "    return masked_im_a[::-1], masked_im_e[::-1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.290212Z",
     "start_time": "2024-06-17T10:25:23.282315Z"
    }
   },
   "id": "7a4d76e179ffd1ba",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_att_images(images_list, save_flag=False, view='a'):\n",
    "    # Define grid size (e.g., 3 rows and 4 columns)\n",
    "    rows = 3\n",
    "    cols = 6\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot each image in the grid\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(images_list):\n",
    "            ax.imshow(images_list[i])\n",
    "        ax.axis('off')  # Turn off axis\n",
    "\n",
    "    # Adjust spacing\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    if not save_flag:\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(f't_{task_id}_d_{traj_id}_{view}.png', dpi=300, bbox_inches='tight')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.294465Z",
     "start_time": "2024-06-17T10:25:23.291216Z"
    }
   },
   "id": "aa50bcb2757f7b9c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_att_maps(task_id, traj_id):\n",
    "    task_embs, agentview_data_list, eye_in_hand_data_list = get_dataset(task_id, traj_id)\n",
    "    agent_att_list = []\n",
    "    eye_att_list = []\n",
    "    for i in range(len(agentview_data_list)):\n",
    "        temp = construct_data(task_embs, agentview_data_list[i], eye_in_hand_data_list[i])\n",
    "        im_a, im_e = show_att_im(temp, model)\n",
    "        agent_att_list.append((im_a * 255).astype(np.uint8))\n",
    "        eye_att_list.append((im_e * 255).astype(np.uint8))\n",
    "\n",
    "    show_att_images(agent_att_list, save_flag=True, view='a')\n",
    "    show_att_images(eye_att_list, save_flag=True, view='e')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:25:23.298485Z",
     "start_time": "2024-06-17T10:25:23.295446Z"
    }
   },
   "id": "802d17acb8db4201",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# display the traj"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81aba0cd2387e535"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "task_id = 0\n",
    "traj_id = 0\n",
    "save_flag = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32bb76c24800f00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "851cdd3cd7a98880"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
